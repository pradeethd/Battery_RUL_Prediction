{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffc3abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7322bdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 6\n",
    "input_size = 7\n",
    "hidden_size = 10\n",
    "num_classes = 1\n",
    "learning_rate = 0.0001\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c97ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatteryDataSet(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        # Data loading\n",
    "        xy = scaled_df_np\n",
    "        self.x = torch.from_numpy(xy[:, 2:-2])\n",
    "        self.y = torch.from_numpy(xy[:, [-1]])\n",
    "        self.n_samples = xy.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # len(Dataset)\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f481a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyer(dataset, batch_size, shuffle_dataset=False):\n",
    "\n",
    "    # get the dataset size\n",
    "    dataset_len = len(dataset)\n",
    "\n",
    "    dataset_size = torch.tensor([dataset_len])\n",
    "\n",
    "    # get the indices\n",
    "    indices = list(range(dataset_len))\n",
    "\n",
    "    # percentage share of data set\n",
    "    # train:        ~ 80 %\n",
    "    # test:         ~ 20 %\n",
    "\n",
    "    # define borders\n",
    "    first_split = int(torch.floor(0.8 * dataset_size))\n",
    "\n",
    "    # set indices for train and test\n",
    "    train_indices = indices[:first_split]\n",
    "    test_indices = indices[first_split:]\n",
    "\n",
    "    # shuffle the dataset\n",
    "    if shuffle_dataset:\n",
    "        np.random.seed()\n",
    "        np.random.shuffle(train_indices)\n",
    "        np.random.shuffle(test_indices)\n",
    "\n",
    "    # set train dataset ot samplers and loader\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "    # set test dataset ot samplers and loader\n",
    "    test_sampler = SubsetRandomSampler(test_indices)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "    return (train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba1b550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, 50)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l3 = nn.Linear(50, 20)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l4 = nn.Linear(20, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l5 = nn.Linear(5, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l5(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ffe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_loop(train_loader, model, loss_fn, optimizer):\n",
    "    # size = len(train_loader)\n",
    "    for batch, (features, RUL) in enumerate(train_loader):\n",
    "\n",
    "        # Forward path\n",
    "        outputs = model(features)\n",
    "        loss = loss_fn(outputs, RUL)\n",
    "\n",
    "        # Backwards path\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if batch % 100 == 0:\n",
    "            # loss, current = loss.item(), batch*len(features)\n",
    "            # print(f'loss: {loss:>7f} [{current:>5d}/{size:>5d}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d477c17",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Test function\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    diff_list = []\n",
    "    targets_list = []\n",
    "    pred_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "            # Difference between prediction and target\n",
    "            diff = abs(y - pred) / y\n",
    "            diff = diff.numpy()\n",
    "            mean_diff = np.mean(diff)\n",
    "            diff_list.append(mean_diff)\n",
    "\n",
    "            # # Target vs prediction\n",
    "            pred_np = pred.squeeze().tolist()\n",
    "            target_np = y.squeeze().tolist()\n",
    "\n",
    "            try:\n",
    "                for i in pred_np:\n",
    "\n",
    "                    pred_list.append(i)\n",
    "                for i in target_np:\n",
    "                    targets_list.append(i)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # Average loss\n",
    "    test_loss /= num_batches\n",
    "\n",
    "    # Average difference\n",
    "    difference_mean = np.mean(diff_list)\n",
    "\n",
    "    # Print the average difference and average loss\n",
    "    print(f\"Test: \\n Avg Difference: {(100*difference_mean):>0.2f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    # Minimum difference and its epoch\n",
    "    min_diff_dict[t+1] = (difference_mean*100)\n",
    "    min_diff_value = min(min_diff_dict.items(), key=lambda x:x[1])\n",
    "    print(\"LOWEST DIFFERENCE AND EPOCH:\")\n",
    "    print(f\"Epoch: {min_diff_value[0]}, diff: {min_diff_value[1]:>0.2f}%\")\n",
    "\n",
    "    # PLOT Target vs Prediction\n",
    "    # if t % 10 == 0:\n",
    "\n",
    "    plt.rcParams[\"figure.dpi\"] = 600\n",
    "    plt.scatter(targets_list, pred_list)\n",
    "    plt.xlabel('Target', fontsize=10)\n",
    "    plt.ylabel('Prediction', fontsize=10)\n",
    "    plt.ylim(0, 1300)\n",
    "    plt.title(f\"Epoch {t+1}\", fontsize=13)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # PLOT Difference\n",
    "\n",
    "    # plt.scatter(t, difference_mean*100)\n",
    "    # plt.ylim(0, 70)\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Target-Pred Difference (%)')\n",
    "    # plt.scatter(t, test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09fd294",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Import data\n",
    "    dataset_raw = pd.read_csv(os.getcwd() + '/Datasets/HNEI_Processed/Final Database.csv')\n",
    "    dataset_raw.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "    # Feature scaling\n",
    "    data = dataset_raw.values[:, :-1]\n",
    "    trans = MinMaxScaler()\n",
    "    data = trans.fit_transform(data)\n",
    "    dataset = pd.DataFrame(data)\n",
    "    dataset_scaled = dataset.join(dataset_raw['RUL'])\n",
    "    scaled_df_np = dataset_scaled.to_numpy(dtype=np.float32)\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = BatteryDataSet()\n",
    "\n",
    "    # Train and test loader\n",
    "    train_loader, test_loader = classifyer(dataset=dataset, batch_size=batch_size\n",
    "                                                         , shuffle_dataset=True)\n",
    "    # Init model\n",
    "    model = NeuralNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "    # Loss function\n",
    "    loss_fn = nn.L1Loss()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Auxiliary dictionary to store epochs and difference values:\n",
    "    min_diff_dict = {}\n",
    "\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "\n",
    "        train_loop(train_loader, model, loss_fn, optimizer)\n",
    "\n",
    "        test_loop(test_loader, model, loss_fn)\n",
    "\n",
    "    print(\"Fertig!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceb62ae",
   "metadata": {},
   "source": [
    "Save model\n",
    "torch.save(NeuralNet.state_dict(), os.getcwd() + '/Datasets/FF_Net_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6954786c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "            "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
